{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "tamil-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of this code is taken from the DeepLearning.ai's Natural Language Processing with Probabilistic Models course assignment ('Autocorrect')\n",
    "# and it is reshaped by centering another txt file as a vocabulary\n",
    "# The text is from Oguz Atay's book: 'Tehlikeli Oyunlar'\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #for tokenization\n",
    "from nltk.stem import PorterStemmer #for stemming\n",
    "import snowballstemmer #We also import this one for \"Turkish\"\n",
    "from nltk.corpus import stopwords \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "registered-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(file_name):\n",
    "    \n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        file_name_d = f.read()\n",
    "    file_name_d_lower = file_name_d.lower() #.lower() for lowering the letters\n",
    "    # tokenize the text word by word\n",
    "    words = word_tokenize(file_name_d_lower)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "floppy-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['oğuz', 'atay', 'tehlikeli', 'oyunlar', 'bütün', 'eserleri/', 'i̇stanbul', 'tehli̇keli̇', 'oyunlar', 'oğuz']\n",
      "There are 27928 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "word_list = processing_data('oguzatay.txt')\n",
    "vocab = set(word_list) # as a vocabulary we choose Oguz Atay : Tehlikeli Oyunlar\n",
    "print(f\"The first ten words in the text are: \\n{word_list[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "outer-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to define get_count function for counting how many times a word appear in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "pressing-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_list):\n",
    "    word_count_dict = {} #fill this with word counts\n",
    "    word_count_dict = Counter(word_list)\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "heard-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27928 key values pairs\n",
      "The count for the word 'başlamak' is 4\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_list)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'başlamak' is {word_count_dict.get('başlamak',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "instructional-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to implement get_probs function which gives you the probability that a word occurs in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "martial-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    probs = {}\n",
    "    cardinality = sum(word_count_dict.values())\n",
    "    for key in word_count_dict.keys():\n",
    "        probs[key] = word_count_dict[key]/cardinality\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "pharmaceutical-above",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 27928\n",
      "P('başlamak') is 0.000027\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('başlamak') is {probs['başlamak']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "average-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    delete_l = [(L+R[1:]) for L, R in split_l if R]\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "conservative-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "\n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    switch_l = [(L + R[1] + R[0] + R[2:]) for L, R in split_l if len(R)>=2]\n",
    "\n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "charitable-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "\n",
    "    letters = 'abcçdefgğhıijklmnoöpqrsştuüvyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    replace_l = [(L+C+R[1:]) for L, R in split_l if len(R)>=1 for C in letters]\n",
    "    replace_set = set(replace_l)\n",
    "    replace_set.discard(word)\n",
    "\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "corresponding-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    \n",
    "    letters = 'abcçdefgğhıijklmnoöpqrsştuüvyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    insert_l = [(L+c+R) for L,R in split_l if 1 for c in letters]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "prepared-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "\n",
    "    edit_one_set = set()\n",
    "    \n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    if allow_switches:\n",
    "        edit_one_set.update(switch_letter(word))\n",
    "    edit_one_set.update(replace_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ranging-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "\n",
    "    edit_two_set = set()\n",
    "\n",
    "    edit_one = edit_one_letter(word,allow_switches=allow_switches)\n",
    "    for w in edit_one:\n",
    "        edit_two = edit_one_letter(w,allow_switches=allow_switches)\n",
    "        edit_two_set.update(edit_two)\n",
    "\n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "smooth-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    suggestions = list((word in vocab and word) or edit_one_letter(word).intersection(vocab) or edit_two_letters(word).intersection(vocab))\n",
    "    \n",
    "    suggestions = list(reversed(suggestions))\n",
    "    for w in suggestions:\n",
    "        n_best.append([w,probs[w]])    \n",
    "\n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "honest-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    \n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): # Replace None with the proper range\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1,n+1): # Replace None with the proper range\n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min(D[row-1,col] + del_cost , D[row,col-1] + ins_cost, D[row-1,col-1] + r_cost)\n",
    "          \n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    med = D[m,n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "injured-envelope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  1 \n",
      "\n",
      "   #  a  n  l  a  t  t  ı\n",
      "#  0  1  2  3  4  5  6  7\n",
      "a  1  0  1  2  3  4  5  6\n",
      "n  2  1  0  1  2  3  4  5\n",
      "l  3  2  1  0  1  2  3  4\n",
      "a  4  3  2  1  0  1  2  3\n",
      "t  5  4  3  2  1  0  1  2\n",
      "ı  6  5  4  3  2  1  2  1\n"
     ]
    }
   ],
   "source": [
    "source =  'anlatı'\n",
    "target = 'anlattı'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "systematic-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word =  başanmak \n",
      "suggestions =  ['boşanmak']\n",
      "word 0: boşanmak, probability 0.000007\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "my_word = 'başanmak' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "nervous-failing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  2 \n",
      "\n",
      "   #  b  o  ş  a  n  m  a  k\n",
      "#  0  1  2  3  4  5  6  7  8\n",
      "b  1  0  1  2  3  4  5  6  7\n",
      "a  2  1  2  3  2  3  4  5  6\n",
      "ş  3  2  3  2  3  4  5  6  7\n",
      "a  4  3  4  3  2  3  4  5  6\n",
      "n  5  4  5  4  3  2  3  4  5\n",
      "m  6  5  6  5  4  3  2  3  4\n",
      "a  7  6  7  6  5  4  3  2  3\n",
      "k  8  7  8  7  6  5  4  3  2\n"
     ]
    }
   ],
   "source": [
    "source =  'başanmak'\n",
    "target = 'boşanmak'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-ecuador",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
