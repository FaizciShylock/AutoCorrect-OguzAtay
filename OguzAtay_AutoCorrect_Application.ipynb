{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cultural-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #for tokenization\n",
    "from nltk.stem import PorterStemmer #for stemming\n",
    "import snowballstemmer #We also import this one for \"Turkish\"\n",
    "from nltk.corpus import stopwords \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "preliminary-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(file_name):\n",
    "    \n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        file_name_d = f.read()\n",
    "    file_name_d_lower = file_name_d.lower() #.lower() for lowering the letters\n",
    "    # tokenize the text word by word\n",
    "    words = word_tokenize(file_name_d_lower)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "genuine-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = processing_data('oguzatay.txt')\n",
    "vocab = set(word_list) # as a vocabulary we choose Oguz Atay : Tehlikeli Oyunlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excess-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to define get_count function for counting how many times a word appear in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absolute-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_list):\n",
    "    word_count_dict = {} #fill this with word counts\n",
    "    word_count_dict = Counter(word_list)\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electoral-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dict = get_count(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aerial-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to implement get_probs function which gives you the probability that a word occurs in a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "weekly-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    probs = {}\n",
    "    cardinality = sum(word_count_dict.values())\n",
    "    for key in word_count_dict.keys():\n",
    "        probs[key] = word_count_dict[key]/cardinality\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "numeric-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = get_probs(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loved-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose=False):\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    delete_l = [(L+R[1:]) for L, R in split_l if R]\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "heavy-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "\n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    switch_l = [(L + R[1] + R[0] + R[2:]) for L, R in split_l if len(R)>=2]\n",
    "\n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "refined-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "\n",
    "    letters = 'abcçdefgğhıijklmnoöpqrsştuüvyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    replace_l = [(L+C+R[1:]) for L, R in split_l if len(R)>=1 for C in letters]\n",
    "    replace_set = set(replace_l)\n",
    "    replace_set.discard(word)\n",
    "\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "corporate-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    \n",
    "    letters = 'abcçdefgğhıijklmnoöpqrsştuüvyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word)+1)]\n",
    "    insert_l = [(L+c+R) for L,R in split_l if 1 for c in letters]\n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "special-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "\n",
    "    edit_one_set = set()\n",
    "    \n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    if allow_switches:\n",
    "        edit_one_set.update(switch_letter(word))\n",
    "    edit_one_set.update(replace_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "proud-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "\n",
    "    edit_two_set = set()\n",
    "\n",
    "    edit_one = edit_one_letter(word,allow_switches=allow_switches)\n",
    "    for w in edit_one:\n",
    "        edit_two = edit_one_letter(w,allow_switches=allow_switches)\n",
    "        edit_two_set.update(edit_two)\n",
    "\n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "australian-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    suggestions = list((word in vocab and word) or edit_one_letter(word).intersection(vocab) or edit_two_letters(word).intersection(vocab))\n",
    "    \n",
    "    suggestions = list(reversed(suggestions))\n",
    "    for w in suggestions:\n",
    "        n_best.append([w,probs[w]])    \n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "comparable-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evden ekmek elmak için Ahmet ila birlakte dışarı çıktım.\n",
      "Did you mean [elma] instead of [elmak] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [olmak] instead of [elmak] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [almak] instead of [elmak] ? [Y/N]Y\n",
      "Did you mean [ima] instead of [ila] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [cila] instead of [ila] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [la] instead of [ila] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [imla] instead of [ila] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [ilaç] instead of [ila] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [isa] instead of [ila] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [ilk] instead of [ila] ? [Y/N]N\n",
      "Hmm... Let me think\n",
      "Did you mean [ile] instead of [ila] ? [Y/N]Y\n",
      "Did you mean [birlikte] instead of [birlakte] ? [Y/N]Y\n",
      "So your corrected text is:\n",
      "  Evden ekmek almak için Ahmet ile birlikte dışarı çıktım .\n"
     ]
    }
   ],
   "source": [
    "text = 'Evden ekmek elmak için Ahmet ila birlakte dışarı çıktım.'\n",
    "text_tokenized = word_tokenize(text)\n",
    "print(text)\n",
    "\n",
    "corrected_text = ''\n",
    "for i in range(len(text_tokenized)):\n",
    "    l_word = text_tokenized[i].lower()\n",
    "    if l_word in vocab:\n",
    "        corrected_text = corrected_text + ' ' + text_tokenized[i]\n",
    "    else:\n",
    "        tmp_corrections = get_corrections(l_word, probs, vocab, 2, verbose=True)\n",
    "        for i, word_prob in enumerate(tmp_corrections):\n",
    "            ans = input(f\"Did you mean [{word_prob[0]}] instead of [{l_word}] ? [Y/N]\")\n",
    "            if ans == 'Y':\n",
    "                corrected_text = corrected_text + ' ' + word_prob[0]\n",
    "                break\n",
    "            else:\n",
    "                print('Hmm... Let me think')\n",
    "\n",
    "\n",
    "print(\"So your corrected text is:\\n\",corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-constraint",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
